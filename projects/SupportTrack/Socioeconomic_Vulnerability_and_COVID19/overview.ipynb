{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Overview of tasks to develop a predictive model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Determine the outcome variable for prediction.\n",
    "\n",
    "Before developing the model, we need to decide what variable the model must predict. It can be a single variable such as number of deaths, number of infections or number of hospitalizations. It can also be a composite of the above variable. For example, if we want to predict the extent to which a particular region is being burderned by COVID-19 we can come up with a new variable named covid_burder = num_deaths + num_infections + num_hospitalizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Load and transform data.\n",
    "\n",
    "Once we have determined the target variable, we need to load the data and combine all the data based on FIPS(County) codes. Then we need to transform the data. Some of the data that we have is time series data, but the timeseries are not of the same length hence we can choose to ignore the time aspect and squish the time axis into one scalar value. Additionally the predictor variable will be a continuous value, we can discretize it into N classes to reduce the resolution for predictions. This will make it easier to fit the model at least initially. Further, these kind of data are most of them times spatially correlated. We need to come up with a transformation that can encode this spatial correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the model.\n",
    "\n",
    "Once the data is transformed it is ready to be fed into the machine learning model. We can try out a variety of models such as Random Forest, Gradient Boosting Trees, Support Vector Machines or Neural Networks. I prefer we used GBM or RF as we can extract feature importance to see which variables have a high importance in predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimize models using hyper param tunning and feature engineering\n",
    "\n",
    "If we have time left after performing the above steps we can look into further improving the models by optimizing the hyperparameters using random search or bayesian optimization and targeted feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('venv': virtualenv)",
   "language": "python",
   "name": "python36964bitvenvvirtualenv43ec2350c1114f53970f4dd0fe9361cc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
